<h1 align="center">Hi ðŸ‘‹, here we have the Advanced_Chapters_of_Neural_Network Laboratories</h1>
<!-- <h3 align="center"> </h3> -->


## Table Of Content
* [Assignment_3: Developed a PyTorch Training Pipeline](#assignment-3)
* [Setup](#setup)
* [How_to_run](#htr)

--------------------------------------------------------------------------------

<h1 id="assignment-3" align="left">Assignment_3: PyTorch Training Pipeline</h1>

<h3 align="left">Here we have the requirement:</h3>

Implement a generic training pipeline using PyTorch. This pipeline must be configurable either through command line arguments or configuration files. Prepare
a short report with the experimental results. The report can be Markdown or PDF file, should have at least 1 page, and will be included in the submission.

<h3 align="left">Specifications:</h3>

1. Pipeline is device agnostic.
2. You can configure classification training on the following datasets: MNIST, CIFAR-10, CIFAR-100, and OxfordIIITPet.
3. Datasets are efficient and support data augmentation.
4. Can use any of the following models: resnet18, resnet50, resnest14d, resnest26d, and MLP. Use timm or huggingface to load the models.
5. Can be configured to use any of the following optimizers: SGD, Adam,
AdamW, Muon, SAM.
6. Can be configured to use one of the following learning rate schedulers: StepLR and ReduceLROnPlateau.
7. Integrates a batch size scheduler.
8. Is integrated with Tensorboard and/or wandb for metrics reporting. Must report relevant training and testing metrics. Supports an early stopping mechanism.

â€¢ Do a hyperparameter sweep using Wandb or a custom script + Tensorboard.

â€¢ Have at least 8 configurations that achieve over 70% accuracy on the CIFAR-100 dataset.

â€¢ Describe the parameters you varied during the sweep in the report.

â€¢ You must mention that the Jax configuration is faster. Add a table with the test accuracy for your experiments and the time spent for training.

â€¢ Include pictures from your metrics reporting system (Tensorboard or wandb).

â€¢ The training pipeline is efficient (training time or RAM usage or VRAM usage).

â€¢ You motivate why in the report and include measurements.

â€¢ You must mention that the Jax configuration is faster. Caching will not be considered, you have to do several other steps to achieve this.


<h3 align="left">How does it work?</h3>

  
<h3 align="left">The logic behind the code:</h3>

The pipeline is based on a reusable and reconfigurable training module. Each experiment is associated with a configuration file based on which the chosen model will be trained. The configurations contain the hyperparameters of the entire pipeline!

Below you will find a structure of the project files which also provides additional explanations:

Assignment_3/
â”‚
â”œâ”€â”€ data/                          # All data-related directories and files
â”‚   â”œâ”€â”€ MNIST/                     # Original, immutable data dump
â”‚   â”œâ”€â”€ CIFAR10/                   # Original, immutable data dump
â”‚   â”œâ”€â”€ CIFAR100/                  # Original, immutable data dump
â”‚   â””â”€â”€ OxfordIIITPet/             # Original, immutable data dump
â”‚
â”œâ”€â”€ docs/                          # Project documentation
â”‚   â”œâ”€â”€ environment.yaml           # Anaconda environment required to setup
â”‚   â”œâ”€â”€ requirements.txt           # Python package requirements file
â”‚   â””â”€â”€ file_structure.txt         # Project files structure
â”‚
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ data_pipeline/             # All modules related to the data pipeline
â”‚   â”‚   â””â”€â”€ preprocessing/         # Data preprocessing modules
â”‚   â”‚       â””â”€â”€ preprocessing.py   # Data preprocessing module
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                    # Neural network models and components
â”‚   â”‚   â”œâ”€â”€ architectures/         # Different neural network architectures
â”‚   â”‚   â””â”€â”€ layers/                # Custom layers
â”‚   â”‚
â”‚   â””â”€â”€ training/
â”‚       â”œâ”€â”€ experiments/           # Specific experiment settings and results
â”‚       â”‚   â”œâ”€â”€ experiment1/       # Each experiment can have its own subdirectory
â”‚       â”‚   â”‚   â”œâ”€â”€ config.yml     # Configuration file for the experiment
â”‚       â”‚   â”‚   â””â”€â”€ results/       # Resulting logs via tensorboard
â”‚       â”‚   â”œâ”€â”€ experiment2/
â”‚       â”‚   â””â”€â”€ ...
â”‚       â”‚
â”‚       â”œâ”€â”€ scripts/               # Actual scripts to run training
â”‚       â”‚   â”œâ”€â”€ wandb              # Wandb logs
â”‚       â”‚   â””â”€â”€ train_model.py     # Main training script
â”‚       â”‚
â”‚       â””â”€â”€ utils/                 # Miscellaneous utilities for training
â”‚           â”œâ”€â”€ weight_initialization.py # Weight initialization strategies
â”‚           â”œâ”€â”€ load_config.py     # Config loader
â”‚           â”œâ”€â”€ get_loss_function.py # Loss Function Getter
â”‚           â”œâ”€â”€ get_lr_scheduler.py # Learning Rate Scheduler Getter
â”‚           â”œâ”€â”€ get_model.py       # Model Getter
â”‚           â”œâ”€â”€ get_optimizer.py   # Optimizer Getter
â”‚           â””â”€â”€ mixed_precision.py # Mixed precision training utilities
â”‚
â””â”€â”€ README.md                      # Overview and instructions for the project
  
---

<h3 id="score" align="left">Best score:</h3>

<img src="https://github.com/user-attachments/assets/1b53b7f2-bc87-4ee5-ae97-14bdf6a11f06" alt="Moments before the disaster" style="width: 300px; height: auto;">

---

<h3 id="setup" align="left">Setup:</h3>

1. Clone the current repositoy! Now you have the project avalable on your local server!</br>
 Type command: ```git clone git@github.com:AndromedaOMA/Neural_Networks---Laboratories.git```
2. Select, open and run the chosen project through PyCharm IDE or the preferred IDE.
3. Have fun!

---

<h3 id="htr" align="left">How to run:</h3>

1. Clone the current repositoy! Now you have the project avalable on your local server!</br>
 Type command: ```git clone git@github.com:AndromedaOMA/Neural_Networks---Laboratories.git```
2. Select, open and run the chosen project through PyCharm IDE or the preferred IDE.
3. Have fun!

---

- âš¡ Fun fact: **tba!**

* [Table Of Content](#table-of-content)

